---
layout: post
title: My Experiences Learning Scrapy to Crawl Web Pages
description: Broadening my horizons by learning to scrape data from the web
---

{{page.title}}
==============

A big challenge for me when writing a web scraper was that it requires a different style of debugging. If you send a request that the website doesn't like then a lot of times you'll get a 404 or redirect page with little indication of what went wrong. If you want to learn more about the tool I used just check out the [Scrapy homepage][scrapy].

I don't have a lot of experience with web programming so I needed some help from a friend. First off Chrome's developer tools and Firefox's Firebug are essential tools for figuring out how web pages work. I'd recommend against using Internet Explorer because many websites handle IE differently from other browsers and probably your scraping tool. Also I don't know if IE even has developer tools built in.

The first step is exploring the code of pages you want to crawl. What information are you trying to scrape? What links do you want to crawl? Does the web pages have Javascript/Flash? I was looking to scrape financial advisor forms from the [SEC's website][sec]. By the way, that link won't work on the weekend. It instead gives the message *"Investment Adviser Public Disclosure is currently unavailable. Please revisit the site during operational hours."* It's absolutely mind boggling when bureaucracy or older institutions try to adjust to technology. Anyways we'll carry on. I've found the html element that will get me to the next page on our crawl.

{% highlight html %}
<input id="ctl00_cphMainContent_ucUnifiedSearch_rdoOrg" type="radio" name="ctl00$cphMainContent$ucUnifiedSearch$rdoSearchBy" value="rdoOrg" onclick="javascript:setTimeout('__doPostBack(\'ctl00$cphMainContent$ucUnifiedSearch$rdoOrg\',\'\')', 0)">
{% endhighlight %}

Alright, there's Javascript, but let's not panic quite yet. Interacting directly with JS is a problem for Scrapy. You'll need something like Selenium for JS, which isn't bad software at all, but it'll be slower since it needs to load the entire page to interact with JS.

<figure>
	<img src="/img/2015-9-12-scrapy/network_tab.png">
	<figcaption>Chrome's network tool shows html request details</figcaption>
</figure>

If we click to the next page on our crawl we can see what Chrome and the server are doing in the background. The Network tab in the Chrome Developer Tools will show us what HTTP requests are going on. I'm specifically looking at the Post request. I notice that the request header is using an **ASP.NET_SessionId** cookie to track our visit. Luckily Scrapy is going to keep track of our cookies for us by default. You can also add cookie information to your logs by putting **COOKIE_DEBUG = True** in your settings.py. The next thing that stands out in the POST request is a form data field.

{% highlight html %}
__EVENTTARGET:ctl00$cphMainContent$ucUnifiedSearch$rdoOrg
__EVENTARGUMENT:
__LASTFOCUS:
__VIEWSTATE:Long gibberish here
__VIEWSTATEGENERATOR:FEB3CE2A
__EVENTVALIDATION:Shorter gibberish here
ctl00$cphMainContent$ucUnifiedSearch$rdoSearchBy:rdoOrg
{% endhighlight %}

Jumping back to the previous page we see this in the html code.

{% highlight html %}
<form method="post" action="iapd_Search.aspx" id="aspnetForm">
	<div>
		<input type="hidden" name="__EVENTTARGET" id="__EVENTTARGET" value="ctl00$cphMainContent$ucUnifiedSearch$rdoOrg">
		<input type="hidden" name="__EVENTARGUMENT" id="__EVENTARGUMENT" value="">
		<input type="hidden" name="__LASTFOCUS" id="__LASTFOCUS" value="">
		<input type="hidden" name="__VIEWSTATE" id="__VIEWSTATE" value="Long gibberish here">
	</div>
		
	<script type="text/javascript">
		//<![CDATA[
		var theForm = document.forms['aspnetForm'];
		if (!theForm) {
	    	theForm = document.aspnetForm;
		}
		function __doPostBack(eventTarget, eventArgument) {
	    	if (!theForm.onsubmit || (theForm.onsubmit() != false)) {
	       		theForm.__EVENTTARGET.value = eventTarget;
	       		theForm.__EVENTARGUMENT.value = eventArgument;
	       		theForm.submit();
	    	}
		}
		//]]>
	</script>

	<div>
		<input type="hidden" name="__VIEWSTATEGENERATOR" id="__VIEWSTATEGENERATOR" value="FEB3CE2A">
		<input type="hidden" name="__EVENTVALIDATION" id="__EVENTVALIDATION" value="Shorter gibberish here">
	</div>
</form>
{% endhighlight %}

That looks familiar! So when we click the radio button element it's sending the \_\_EVENTTARGET and a blank \_\_EVENTARGUMENT to the JS. There's also a large mess in \_\_VIEWSTATE. I had originally put that in my code which looked really ugly and apparently wasn't necessary because using **scrapy.FormRequest.from_response()** will autofill a form based on the response data of the previous request. You can then override or add fields because it stores the formdata as a dict. I discovered [this][aspnet] in the Scrapy faq to help me understand what ASP.Net was doing in the background. I end up with this:

{% highlight python %}
return scrapy.FormRequest.from_response(response,
    url="http://www.adviserinfo.sec.gov/IAPD/Content/Search/iapd_Search.aspx",
    formdata={'__EVENTTARGET': 'ctl00$cphMainContent$ucUnifiedSearch$rdoOrg', 
              'ctl00$cphMainContent$ucUnifiedSearch$rdoSearchBy': 'rdoOrg'},
    callback=self.callback_function)
{% endhighlight %}

Which is pretty slick! After that I just duplicated this investigation process for subsequent pages and wrote xpath selectors to extract the information I wanted. Web crawling and scraping is a useful tool because sometimes you'll need to go out and get your own data. Not everything is as simple as going to UC's machine learning archive or downloading a CSV.

[scrapy]: http://scrapy.org/
[sec]: http://www.adviserinfo.sec.gov/IAPD/Content/Search/iapd_Search.aspx
[aspnet]: http://doc.scrapy.org/en/latest/faq.html#what-s-this-huge-cryptic-viewstate-parameter-used-in-some-forms